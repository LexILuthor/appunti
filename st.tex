\documentclass[fontsize=11pt,paper=A4,oneside,index=totoc,hyperref]{book}


\usepackage[italian]{babel}
\usepackage[utf8]{inputenc} % Uses the utf8 input encoding
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs

%\usepackage{booktabs,array} % Packages for tables

\usepackage{amsmath} % For typesetting math
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage[osf]{mathpazo} % Palatino as the main font

\theoremstyle{definition}
\newtheorem{dfn}{Definizione}[]

\theoremstyle{plain}
\newtheorem{thm}{Teorema}[section]
\newtheorem{prp}{Proposizione}[section]
\newtheorem{exe}{Esercizio}[section]

\newcommand{\Asp}{\mathbb{E}}
\newcommand{\Ind}[1]{\mathbb{1}_{#1}}

\DeclareMathOperator{\Var}{Var}

\newcommand\restr[2]{{% we make the whole thing an ordinary symbol
  \left.\kern-\nulldelimiterspace % automatically resize the bar with \right
  #1 % the function
  \vphantom{\big|} % pretend it's a little taller at normal size
  \right|_{#2} % this is the delimiter
  }}

% These lines make it so each new lab day directly follows the previous one i.e. does not start on a new page - comment them out to separate lab days on new pages
\makeatletter
%\patchcmd{\addchap}{\if@openright\cleardoublepage\else\clearpage\fi}{\par}{}{}
\makeatother

% Hyperlink configuration
\usepackage[
    pdfauthor={Filippo}, % Your name for the author field in the PDF
    pdftitle={Statistica Matematica}, % PDF title
    pdfsubject={Statistica}, % PDF subject
    bookmarksopen=true,
    linktocpage=true,
    urlcolor=green, % Color of URLs
    citecolor=green, % Color of citations
    linkcolor=green, % Color of links to other pages/figures
    backref=page,
    pdfpagelabels=true,
    plainpages=false,
    colorlinks=true, % Turn off all coloring by changing this to false
    bookmarks=true,
    pdfview=FitB]{hyperref}

\usepackage[stretch=10]{microtype} % Slightly tweak font spacing for aesthetics

\begin{document}

\title{Appunti di Statistica}

\author{
    Uno a caso \\[1cm]% Your name
    Corso di Matematica % Your degree
}
\date{\today} % No date by default, add \today if you wish to include the publication date

\maketitle % Title page

\tableofcontents % Table of contents

\chapter{Le prime lezioni.}

\section{Richiami.}

\begin{dfn}[Variabile casuale]
  Dato uno spazio degli eventi \(\Omega\), la funzione \(X \colon \Omega \to \mathbb{R}\) che assegna a ciascun evento \(\omega \in \Omega\) un numero reale \(x = X(\omega)\) è detta \emph{variabile casuale}. Lo \emph{spazio} di \(X\) è l'insieme dei numeri reali \(\lbrace x \colon x = X(\omega), \omega \in \Omega\rbrace\).
\end{dfn}

Nel caso in cui il supporto sia un insieme numerabile la variabile si dice \emph{discreta}, se invece esso è costituito da un intervallo di numeri reali la variabile si dice \emph{continua}.

\begin{dfn}[Funzione di probabilità]
  Data una variabile casuale discreta \(X \colon \Omega \to \mathbb{R}\) avente spazio \(A\), la \emph{funzione di massa probabilistica} ({\bf pmf}) di \(X\) è definita come
  \[
  p_X(x) = P(X = x),\quad x \in A
  \]
  e soddisfa alle seguenti condizioni:
  \begin{enumerate}
    \item \(\forall x \in A.\quad 0 \le p_X(x) \le 1\);
    \item \(\sum_{x\in A}p_X(x) = 1\).
  \end{enumerate}
\end{dfn}

\begin{dfn}[Distribuzione di probabilità]
  Data una variabile casuale \(X\) avente spazio \(A\) funzione di massa probabilistica \(p_X\), è possibile indurre una \emph{distribuzione di probabilità}
  \begin{equation}
    P_X(B) = \sum_{b\in B}p_X(b), B \subset A
  \end{equation}
  che costituisce una probabilità su \(A\).
\end{dfn}

\begin{dfn}[Funzione di ripartizione]
  Data una variabile casuale \(X\), la sua \emph{funzione di distribuzione} (o \emph{funzione di ripartizione}) è una funzione data da
  \begin{equation}
    F_X(x) = P_X((-\infty,x]) = P(X \le x)
  \end{equation}
  tale da soddisfare alle seguenti condizioni:
  \begin{enumerate}
    \item se \(a<b\), allora \(F_X(a) \le F_X(b)\), ovvero è non-decrescente;
    \item \(\lim_{x\to-\infty}F_X(x) = 0\);
    \item \(\lim_{x\to\infty}F_X(x) = 1\);
    \item \(\lim_{x\to x_0^{+}} F_X(x) = F_X(x_0)\), ossia è una funzione continua da destra.
  \end{enumerate}
\end{dfn}

\begin{dfn}[Assoluta continuità]
  Una variabile casuale continua \(X\) si dice essere \emph{assolutamente continua} se esiste una funzione \(f_X\) tale che
  \begin{equation}
    F_X(x) = \int_{-\infty}^x f_X(t) \mathrm{d}t.
  \end{equation}
  La funzione \(f_X(t)\) è detta \emph{densità di probabilità} di \(X\); nel caso in cui essa sia continua, \(F_X^\prime(x) = f_X(x)\). Una densità di probabilità \(f_X(x)\) è tale da soddisfare alle seguenti condizioni:
  \begin{enumerate}
    \item \(\forall x \in A.\quad f_X(x) > 0\);
    \item \(\int_{-\infty}^\infty f_X(x) \mathrm{d}x = 1\).
  \end{enumerate}
\end{dfn}

\begin{dfn}[Supporto]
  Si dice \emph{supporto} di una variabile casuale discreta (risp. continua) l'insieme dei punti \(x\) per cui \(p_X(x) > 0\) (risp. \(f_X(x) > 0\)).
\end{dfn}

\begin{dfn}[Aspettazione]
  Sia \(X\) una variabile casuale. Se \(X\) è continua con densità di probabilità \(f\) e
  \[
  \int_{-\infty}^{\infty}\lvert x\rvert f(x) \mathrm{d}x < \infty
  \]
  allora il \emph{valore di aspettazione} di \(X\) è \(\Asp(X) = \int_{-\infty}^{\infty}xf(x)\mathrm{d}x\). Se \(X\) è discreta con funzione di massa probabilistica \(p(x)\) e
  \[
  \sum_x \lvert x\rvert p(x) < \infty,
  \]
  allora il valore di aspettazione di \(X\) è \(\Asp(X) = \sum_x xp(x)\).
\end{dfn}

Una delle possibili accezioni di \(\Asp(x)\) è quella di \emph{media} di \(X\): solitamente, quando si vuole intendere tale significato essa viene indicata come \(\mu = \Asp(X)\). Ricordiamo che \(\Asp(X)\) è un operatore lineare e che lascia invariati i valori costanti.

% TO-DO: Definizione di varianza.

\section{Momenti. Funzione generatrice}
\begin{dfn}[Momento]
  Data una variabile casuale \(X \sim F(x, {\boldsymbol{\theta}})\) si dice \emph{momento} di ordine \(s\) centrato in \(a\) la funzione
  \[
  \mu_s(a) = \Asp((X - a)^s),\quad s \in \mathbb{N}^{*}
  \]
\end{dfn}
Nel caso in cui \(X\) fosse una variabile casuale discreta, allora
\[
\Asp((X-a)^s) = \sum_{x}(x-a)^sP_a(X = x).
\]
mentre nel caso continuo
\[
\Asp((X-a)^s) = \int_{-\infty}^{+\infty}(x-a)^s f(x, {\boldsymbol{\theta}}) \Ind{A(x)}\mathrm{d}x
\]

\paragraph{Momenti particolari.} Ponendo \(a = 0\), si ottiene la classe dei momenti centrati nell'origine di ordine \(s\)
\[
\mu_s^\prime = \Asp(X^s)
\]
di cui fa parte \(\mu = \Asp(X) = \mu_1^\prime\). Ponendo \(a = \mu\) si possono ottenere alcuni momenti "notevoli":
\begin{align*}
  \mu_2 &= \Asp(X - \mu)^2 = \sigma^2 \\
  \mu_3 &= \Asp(X - \mu)^3 \\
  \mu_4 &= \Asp(X - \mu)^4
\end{align*}
il significato statistico di tali momenti è il seguente: \(\mu_2\) indica la varianza di \(X\), \(\mu_3\) dà una cifra della {\bf simmetria} della distribuzione di \(X\) mentre \(\mu_4\) ne rappresenta la {\bf curtosi}, ovvero la stima del "peso" delle code della distribuzione.

Possiamo mettere in relazione i momenti centrati con i momenti non centrati di una variabile casuale nel seguente modo:
\begin{equation}
  \mu_s = \Asp((X - \mu_1^\prime)^s) = \Asp((X - \mu)^s) = \sum_{m=0}^s(-1)^m
\binom{s}{m}{\mu^\prime}^{s - m}\mu^m.
\end{equation}

\begin{dfn}[Funzione generatrice dei momenti.]
  Sia \(X\) una variabile casuale, discreta o assolutamente continua. Se esiste \(t_0 > 0\) tale che
  \begin{equation}
    \forall t \in (-t_0,t_0).\quad \Asp(e^{tX}) < \infty
  \end{equation}
  diremo la funzione \(M_X(t) = \Asp(e^{tX})\) \emph{funzione generatrice dei momenti} ({\bf fgm}) di \(X\).
\end{dfn}


\section{Trasformazioni di variabili casuali.}

\begin{thm}[Trasformazione di variabili continue]
  Sia \(X\) una variabile casuale continua con densita \(f_X\) e sia \(W = h(X)\), dove \(h\) è una funzione monotona di \(X\). Supponiamo che \(f_X\) sia continua sul supporto di \(X\) e che \(h^{-1}(W)\) abbia derivate continue sul supporto di \(W\). Allora,
  \begin{equation}
    \begin{cases}
      f_W(w) = f_X(h^{-1}(w))\cdot\left\lvert \frac{\mathrm{d}}{\mathrm{d}w}h^{-1}(w)\right\rvert & w \text{ nel supporto di } W \\ 0 & \text{altrove}
    \end{cases} \label{tra1}
  \end{equation}
\end{thm}
\begin{exe}
  Sia \(X \sim N(\mu,\sigma^2)\) e sia \(Z = \tfrac{X - \mu}{\sigma}\) la \emph{normalizzazione} di \(X\). Trovare la distribuzione di \(Z\).
\end{exe}
\begin{proof}[Soluzione]
  Si definisca \(h(X) = \tfrac{X - \mu}{\sigma}\): essa è una funzione monotona crescente; inoltre, si noti che \(X = \mu + \sigma Z\). Applicando \eqref{tra1} si ottiene dunque
  \[
  f_Z(z) = \frac{1}{\sqrt{2\pi}\sigma}e^{-z^2}\sigma\Ind{\mathbb{R}}(z).
  \]
  Notiamo che \(Z\) è distribuita normalmente, similmente a \(X\): i suoi parametri corrispondono a
  \begin{align*}
    \Asp(Z) &= \Asp\left(\frac{X-\mu}{\sigma}\right) = \frac{\Asp(X-\mu)}{\sigma} = 0; \\
    \Var(Z) &= \Var\left(\frac{X-\mu}{\sigma}\right) = \frac{\Var(X-\mu)}{\sigma^2} = \\ &= \frac{\Var(X)}{\sigma^2} = 1.
  \end{align*}
  Dunque, \(Z \sim N(0,1)\).
\end{proof}

\paragraph{Circa la monotonia.} Nel caso \(h(X)\) fosse una funzione non monotona ma \emph{monotona a tratti}, se le altre condizioni del teorema sono soddisfatte vale
\begin{equation}
  f_W(w) = \sum_{m=1}^k f_X(h_m^{-1}(w))\cdot\left\lvert\frac{\mathrm{d}}{\mathrm{d}w}h_m^{-1}(w)\right\rvert \Ind{\mathcal{S}_W}(w).
\end{equation}
dove \(\mathcal{S}_W\) indica il supporto di \(W\).

\begin{exe}
  Sia \(Z \sim N(0,1)\) con \(f_Z(z) = \tfrac{1}{\sqrt{2\pi}}e^{-\tfrac{z^2}{2}}\Ind{\mathbb{R}}(z)\) e sia \(W = h(Z) = Z^2\). Trovare la distribuzione di \(W\).
\end{exe}
\begin{proof}[Soluzione]
  La funzione \(h\) è monotona in tre tratti \(A_0 = \lbrace 0 \rbrace\), \(A_1 = (-\infty,0)\) e \(A_2 = (0,\infty)\). Andando a calcolare su ciascuno dei tratti i valori a noi utili,
  \begin{align*}
    h_1^{-1}(w) = -\sqrt{w}&; \frac{\mathrm{d}}{\mathrm{d}w}h_1^{-1}(w) = -\frac{1}{2\sqrt{w}} \\
    h_2^{-1}(w) = \sqrt{w}&; \frac{\mathrm{d}}{\mathrm{d}w}h_2^{-1}(w) = \frac{1}{2\sqrt{w}}.
  \end{align*}
  Applicando la formula di trasformazione si ottiene
  \[
  f_W(w) = \frac{1}{\sqrt{2\pi}}\frac{1}{\sqrt{w}}e^{-\frac12 w}\Ind{\mathbb{R}^+}(w)
  \]
  ossia \(W \sim \mathcal{G}(\alpha = \frac12, \beta = 2) = \chi_1^2\), ossia \(W\) ha distribuzione \emph{chi-quadrato} con grado di libertà \(1\).
\end{proof}
\paragraph{Riguardo al chi-quadrato.} Si consideri una famiglia di \(n\) variabili casuali normali standard ed indipendenti e si definisca \(S_n = \sum_{i=1}^n Z_i\). Poiché \(M_{S_n}(t) = (M_{Z_1}(t))^n = \left(\tfrac{1}{\sqrt{1-2t}}\right)^n\), si ha
\begin{equation}
  S_n \sim \mathcal{G}(\alpha = \frac{n}{2}, \beta = 2) = \chi_n^2
\end{equation}
ossia la somma di variabili casuali normali standard indipendenti ha distribuzione chi-quadrato con \(n\) gradi di libertà.
\end{document}
